{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this notebook compatible for Python 2 and 3\n",
    "from __future__ import division, print_function\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "import progressbar\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to import module from parent directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Dataset API from sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_learning.optimizers import Adam\n",
    "from deep_learning.loss_functions import CrossEntropy, SquareLoss\n",
    "from deep_learning.layers import Dense, Dropout, Flatten, Activation, Reshape, BatchNormalization \n",
    "from deep_learning.neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+-------------------------+\n",
      "| Variational Autoencoder |\n",
      "+-------------------------+\n",
      "Input Shape: (784,)\n",
      "+------------------------+------------+--------------+\n",
      "| Layer Type             | Parameters | Output Shape |\n",
      "+------------------------+------------+--------------+\n",
      "| Dense                  | 401920     | (512,)       |\n",
      "| Activation (LeakyReLU) | 0          | (512,)       |\n",
      "| BatchNormalization     | 1024       | (512,)       |\n",
      "| Dense                  | 131328     | (256,)       |\n",
      "| Activation (LeakyReLU) | 0          | (256,)       |\n",
      "| BatchNormalization     | 512        | (256,)       |\n",
      "| Dense                  | 32896      | (128,)       |\n",
      "| Dense                  | 33024      | (256,)       |\n",
      "| Activation (LeakyReLU) | 0          | (256,)       |\n",
      "| BatchNormalization     | 512        | (256,)       |\n",
      "| Dense                  | 131584     | (512,)       |\n",
      "| Activation (LeakyReLU) | 0          | (512,)       |\n",
      "| BatchNormalization     | 1024       | (512,)       |\n",
      "| Dense                  | 402192     | (784,)       |\n",
      "| Activation (TanH)      | 0          | (784,)       |\n",
      "+------------------------+------------+--------------+\n",
      "Total Parameters: 1136016\n",
      "\n",
      "0 [D loss: 0.562230]\n",
      "1 [D loss: 0.553967]\n",
      "2 [D loss: 0.545364]\n",
      "3 [D loss: 0.537212]\n",
      "4 [D loss: 0.529560]\n",
      "5 [D loss: 0.521411]\n",
      "6 [D loss: 0.515279]\n",
      "7 [D loss: 0.509218]\n",
      "8 [D loss: 0.503353]\n",
      "9 [D loss: 0.493033]\n",
      "10 [D loss: 0.496837]\n",
      "11 [D loss: 0.489696]\n",
      "12 [D loss: 0.482826]\n",
      "13 [D loss: 0.474383]\n",
      "14 [D loss: 0.479328]\n",
      "15 [D loss: 0.470744]\n",
      "16 [D loss: 0.469344]\n",
      "17 [D loss: 0.469268]\n",
      "18 [D loss: 0.464187]\n",
      "19 [D loss: 0.456459]\n",
      "20 [D loss: 0.455769]\n",
      "21 [D loss: 0.458676]\n",
      "22 [D loss: 0.458963]\n",
      "23 [D loss: 0.452444]\n",
      "24 [D loss: 0.455671]\n",
      "25 [D loss: 0.449387]\n",
      "26 [D loss: 0.441510]\n",
      "27 [D loss: 0.441853]\n",
      "28 [D loss: 0.449701]\n",
      "29 [D loss: 0.441719]\n",
      "30 [D loss: 0.447992]\n",
      "31 [D loss: 0.454855]\n",
      "32 [D loss: 0.438114]\n",
      "33 [D loss: 0.430646]\n",
      "34 [D loss: 0.438529]\n",
      "35 [D loss: 0.436365]\n",
      "36 [D loss: 0.431767]\n",
      "37 [D loss: 0.432432]\n",
      "38 [D loss: 0.425937]\n",
      "39 [D loss: 0.423368]\n",
      "40 [D loss: 0.425900]\n",
      "41 [D loss: 0.420741]\n",
      "42 [D loss: 0.430781]\n",
      "43 [D loss: 0.435355]\n",
      "44 [D loss: 0.432739]\n",
      "45 [D loss: 0.418410]\n",
      "46 [D loss: 0.422911]\n",
      "47 [D loss: 0.423208]\n",
      "48 [D loss: 0.420242]\n",
      "49 [D loss: 0.421002]\n",
      "50 [D loss: 0.418339]\n",
      "51 [D loss: 0.416392]\n",
      "52 [D loss: 0.410856]\n",
      "53 [D loss: 0.419553]\n",
      "54 [D loss: 0.417417]\n",
      "55 [D loss: 0.412535]\n",
      "56 [D loss: 0.412952]\n",
      "57 [D loss: 0.413215]\n",
      "58 [D loss: 0.408415]\n",
      "59 [D loss: 0.416310]\n",
      "60 [D loss: 0.414038]\n",
      "61 [D loss: 0.415505]\n",
      "62 [D loss: 0.415283]\n",
      "63 [D loss: 0.402539]\n",
      "64 [D loss: 0.406906]\n",
      "65 [D loss: 0.401272]\n",
      "66 [D loss: 0.415822]\n",
      "67 [D loss: 0.403032]\n",
      "68 [D loss: 0.398924]\n",
      "69 [D loss: 0.404292]\n",
      "70 [D loss: 0.405586]\n",
      "71 [D loss: 0.403338]\n",
      "72 [D loss: 0.400115]\n",
      "73 [D loss: 0.394706]\n",
      "74 [D loss: 0.401319]\n",
      "75 [D loss: 0.407858]\n",
      "76 [D loss: 0.395853]\n",
      "77 [D loss: 0.413286]\n",
      "78 [D loss: 0.402700]\n",
      "79 [D loss: 0.395453]\n",
      "80 [D loss: 0.393646]\n",
      "81 [D loss: 0.392139]\n",
      "82 [D loss: 0.408048]\n",
      "83 [D loss: 0.400006]\n",
      "84 [D loss: 0.398547]\n",
      "85 [D loss: 0.415301]\n",
      "86 [D loss: 0.396438]\n",
      "87 [D loss: 0.399755]\n",
      "88 [D loss: 0.398055]\n",
      "89 [D loss: 0.395158]\n",
      "90 [D loss: 0.398954]\n",
      "91 [D loss: 0.388857]\n",
      "92 [D loss: 0.394654]\n",
      "93 [D loss: 0.397323]\n",
      "94 [D loss: 0.414021]\n",
      "95 [D loss: 0.396097]\n",
      "96 [D loss: 0.385794]\n",
      "97 [D loss: 0.388798]\n",
      "98 [D loss: 0.399403]\n",
      "99 [D loss: 0.390928]\n",
      "100 [D loss: 0.388620]\n",
      "101 [D loss: 0.384501]\n",
      "102 [D loss: 0.382879]\n",
      "103 [D loss: 0.382029]\n",
      "104 [D loss: 0.381972]\n",
      "105 [D loss: 0.382572]\n",
      "106 [D loss: 0.376823]\n",
      "107 [D loss: 0.381409]\n",
      "108 [D loss: 0.370846]\n",
      "109 [D loss: 0.380729]\n",
      "110 [D loss: 0.380973]\n",
      "111 [D loss: 0.377179]\n",
      "112 [D loss: 0.376160]\n",
      "113 [D loss: 0.385786]\n",
      "114 [D loss: 0.377086]\n",
      "115 [D loss: 0.377285]\n",
      "116 [D loss: 0.378330]\n",
      "117 [D loss: 0.387051]\n",
      "118 [D loss: 0.371915]\n",
      "119 [D loss: 0.379422]\n",
      "120 [D loss: 0.372522]\n",
      "121 [D loss: 0.371335]\n",
      "122 [D loss: 0.374175]\n",
      "123 [D loss: 0.373139]\n",
      "124 [D loss: 0.374192]\n",
      "125 [D loss: 0.365552]\n",
      "126 [D loss: 0.373017]\n",
      "127 [D loss: 0.367384]\n",
      "128 [D loss: 0.373246]\n",
      "129 [D loss: 0.364657]\n",
      "130 [D loss: 0.367779]\n",
      "131 [D loss: 0.365513]\n",
      "132 [D loss: 0.371138]\n",
      "133 [D loss: 0.376942]\n",
      "134 [D loss: 0.371565]\n",
      "135 [D loss: 0.372263]\n",
      "136 [D loss: 0.372761]\n",
      "137 [D loss: 0.376904]\n",
      "138 [D loss: 0.374827]\n",
      "139 [D loss: 0.370882]\n",
      "140 [D loss: 0.371670]\n",
      "141 [D loss: 0.367614]\n",
      "142 [D loss: 0.360436]\n",
      "143 [D loss: 0.359767]\n",
      "144 [D loss: 0.367061]\n",
      "145 [D loss: 0.371356]\n",
      "146 [D loss: 0.358220]\n",
      "147 [D loss: 0.365250]\n",
      "148 [D loss: 0.361290]\n",
      "149 [D loss: 0.359468]\n",
      "150 [D loss: 0.353105]\n",
      "151 [D loss: 0.363183]\n",
      "152 [D loss: 0.360483]\n",
      "153 [D loss: 0.367455]\n",
      "154 [D loss: 0.357037]\n",
      "155 [D loss: 0.362254]\n",
      "156 [D loss: 0.350756]\n",
      "157 [D loss: 0.352329]\n",
      "158 [D loss: 0.366236]\n",
      "159 [D loss: 0.359317]\n",
      "160 [D loss: 0.353485]\n",
      "161 [D loss: 0.359435]\n",
      "162 [D loss: 0.360954]\n",
      "163 [D loss: 0.358347]\n",
      "164 [D loss: 0.371556]\n",
      "165 [D loss: 0.350897]\n",
      "166 [D loss: 0.363227]\n",
      "167 [D loss: 0.367759]\n",
      "168 [D loss: 0.355781]\n",
      "169 [D loss: 0.355397]\n",
      "170 [D loss: 0.355806]\n",
      "171 [D loss: 0.364520]\n",
      "172 [D loss: 0.359764]\n",
      "173 [D loss: 0.371410]\n",
      "174 [D loss: 0.357812]\n",
      "175 [D loss: 0.361635]\n",
      "176 [D loss: 0.346591]\n",
      "177 [D loss: 0.347112]\n",
      "178 [D loss: 0.347512]\n",
      "179 [D loss: 0.358855]\n",
      "180 [D loss: 0.354566]\n",
      "181 [D loss: 0.367693]\n",
      "182 [D loss: 0.345134]\n",
      "183 [D loss: 0.353517]\n",
      "184 [D loss: 0.346558]\n",
      "185 [D loss: 0.347383]\n",
      "186 [D loss: 0.388042]\n",
      "187 [D loss: 0.344624]\n",
      "188 [D loss: 0.351119]\n",
      "189 [D loss: 0.350944]\n",
      "190 [D loss: 0.353820]\n",
      "191 [D loss: 0.356637]\n",
      "192 [D loss: 0.365351]\n",
      "193 [D loss: 0.351914]\n",
      "194 [D loss: 0.349629]\n",
      "195 [D loss: 0.338129]\n",
      "196 [D loss: 0.352266]\n",
      "197 [D loss: 0.359668]\n",
      "198 [D loss: 0.349466]\n",
      "199 [D loss: 0.344975]\n",
      "200 [D loss: 0.370179]\n",
      "201 [D loss: 0.359428]\n",
      "202 [D loss: 0.349697]\n",
      "203 [D loss: 0.348725]\n",
      "204 [D loss: 0.336530]\n",
      "205 [D loss: 0.352760]\n",
      "206 [D loss: 0.348962]\n",
      "207 [D loss: 0.356486]\n",
      "208 [D loss: 0.335537]\n",
      "209 [D loss: 0.342732]\n",
      "210 [D loss: 0.337006]\n",
      "211 [D loss: 0.342547]\n",
      "212 [D loss: 0.341216]\n",
      "213 [D loss: 0.335157]\n",
      "214 [D loss: 0.336817]\n",
      "215 [D loss: 0.351614]\n",
      "216 [D loss: 0.345918]\n",
      "217 [D loss: 0.341745]\n",
      "218 [D loss: 0.337482]\n",
      "219 [D loss: 0.336457]\n",
      "220 [D loss: 0.346330]\n",
      "221 [D loss: 0.342438]\n",
      "222 [D loss: 0.333919]\n",
      "223 [D loss: 0.329682]\n",
      "224 [D loss: 0.341744]\n",
      "225 [D loss: 0.333857]\n",
      "226 [D loss: 0.336769]\n",
      "227 [D loss: 0.330131]\n",
      "228 [D loss: 0.340024]\n",
      "229 [D loss: 0.333031]\n",
      "230 [D loss: 0.332108]\n",
      "231 [D loss: 0.341124]\n",
      "232 [D loss: 0.358756]\n",
      "233 [D loss: 0.331437]\n",
      "234 [D loss: 0.334142]\n",
      "235 [D loss: 0.341275]\n",
      "236 [D loss: 0.341751]\n",
      "237 [D loss: 0.323389]\n",
      "238 [D loss: 0.331890]\n",
      "239 [D loss: 0.347008]\n",
      "240 [D loss: 0.344980]\n",
      "241 [D loss: 0.340329]\n",
      "242 [D loss: 0.336414]\n",
      "243 [D loss: 0.322315]\n",
      "244 [D loss: 0.338344]\n",
      "245 [D loss: 0.329459]\n",
      "246 [D loss: 0.331564]\n",
      "247 [D loss: 0.323124]\n",
      "248 [D loss: 0.334108]\n",
      "249 [D loss: 0.325135]\n",
      "250 [D loss: 0.325201]\n",
      "251 [D loss: 0.331951]\n",
      "252 [D loss: 0.341776]\n",
      "253 [D loss: 0.319334]\n",
      "254 [D loss: 0.331613]\n",
      "255 [D loss: 0.338145]\n",
      "256 [D loss: 0.327049]\n",
      "257 [D loss: 0.331050]\n",
      "258 [D loss: 0.336233]\n",
      "259 [D loss: 0.340087]\n",
      "260 [D loss: 0.328720]\n",
      "261 [D loss: 0.326249]\n",
      "262 [D loss: 0.324143]\n",
      "263 [D loss: 0.324151]\n",
      "264 [D loss: 0.330218]\n",
      "265 [D loss: 0.334083]\n",
      "266 [D loss: 0.332645]\n",
      "267 [D loss: 0.340081]\n",
      "268 [D loss: 0.319663]\n",
      "269 [D loss: 0.318110]\n",
      "270 [D loss: 0.327033]\n",
      "271 [D loss: 0.342727]\n",
      "272 [D loss: 0.340406]\n",
      "273 [D loss: 0.324580]\n",
      "274 [D loss: 0.326084]\n",
      "275 [D loss: 0.328773]\n",
      "276 [D loss: 0.327161]\n",
      "277 [D loss: 0.332554]\n",
      "278 [D loss: 0.331183]\n",
      "279 [D loss: 0.326499]\n",
      "280 [D loss: 0.326162]\n",
      "281 [D loss: 0.324981]\n",
      "282 [D loss: 0.323971]\n",
      "283 [D loss: 0.329935]\n",
      "284 [D loss: 0.327199]\n",
      "285 [D loss: 0.336532]\n",
      "286 [D loss: 0.328959]\n",
      "287 [D loss: 0.312926]\n",
      "288 [D loss: 0.324897]\n",
      "289 [D loss: 0.327447]\n",
      "290 [D loss: 0.314696]\n",
      "291 [D loss: 0.321304]\n",
      "292 [D loss: 0.323912]\n",
      "293 [D loss: 0.329150]\n",
      "294 [D loss: 0.323397]\n",
      "295 [D loss: 0.321996]\n",
      "296 [D loss: 0.322014]\n",
      "297 [D loss: 0.371599]\n",
      "298 [D loss: 0.324347]\n",
      "299 [D loss: 0.325490]\n",
      "300 [D loss: 0.328855]\n",
      "301 [D loss: 0.321105]\n",
      "302 [D loss: 0.328456]\n",
      "303 [D loss: 0.326085]\n",
      "304 [D loss: 0.329357]\n",
      "305 [D loss: 0.324305]\n",
      "306 [D loss: 0.320789]\n",
      "307 [D loss: 0.314927]\n",
      "308 [D loss: 0.330365]\n",
      "309 [D loss: 0.324980]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 [D loss: 0.330981]\n",
      "311 [D loss: 0.314899]\n",
      "312 [D loss: 0.330760]\n",
      "313 [D loss: 0.324923]\n",
      "314 [D loss: 0.332062]\n",
      "315 [D loss: 0.314893]\n",
      "316 [D loss: 0.321148]\n",
      "317 [D loss: 0.318560]\n",
      "318 [D loss: 0.322350]\n",
      "319 [D loss: 0.314662]\n",
      "320 [D loss: 0.308637]\n",
      "321 [D loss: 0.323957]\n",
      "322 [D loss: 0.317947]\n",
      "323 [D loss: 0.317813]\n",
      "324 [D loss: 0.309265]\n",
      "325 [D loss: 0.320596]\n",
      "326 [D loss: 0.319668]\n",
      "327 [D loss: 0.310394]\n",
      "328 [D loss: 0.317836]\n",
      "329 [D loss: 0.314485]\n",
      "330 [D loss: 0.312452]\n",
      "331 [D loss: 0.313244]\n",
      "332 [D loss: 0.326952]\n",
      "333 [D loss: 0.308411]\n",
      "334 [D loss: 0.312797]\n",
      "335 [D loss: 0.310896]\n",
      "336 [D loss: 0.316527]\n",
      "337 [D loss: 0.315982]\n",
      "338 [D loss: 0.318197]\n",
      "339 [D loss: 0.317702]\n",
      "340 [D loss: 0.304638]\n",
      "341 [D loss: 0.313145]\n",
      "342 [D loss: 0.319059]\n",
      "343 [D loss: 0.315048]\n",
      "344 [D loss: 0.307954]\n",
      "345 [D loss: 0.305742]\n",
      "346 [D loss: 0.319870]\n",
      "347 [D loss: 0.322255]\n",
      "348 [D loss: 0.310462]\n",
      "349 [D loss: 0.320925]\n",
      "350 [D loss: 0.307745]\n",
      "351 [D loss: 0.307101]\n",
      "352 [D loss: 0.325724]\n",
      "353 [D loss: 0.317045]\n",
      "354 [D loss: 0.321993]\n",
      "355 [D loss: 0.310850]\n",
      "356 [D loss: 0.317621]\n",
      "357 [D loss: 0.302967]\n",
      "358 [D loss: 0.310698]\n",
      "359 [D loss: 0.311982]\n",
      "360 [D loss: 0.319692]\n",
      "361 [D loss: 0.322116]\n",
      "362 [D loss: 0.327367]\n",
      "363 [D loss: 0.316305]\n",
      "364 [D loss: 0.311637]\n",
      "365 [D loss: 0.322911]\n",
      "366 [D loss: 0.305798]\n",
      "367 [D loss: 0.313194]\n",
      "368 [D loss: 0.308090]\n",
      "369 [D loss: 0.328257]\n",
      "370 [D loss: 0.301957]\n",
      "371 [D loss: 0.334455]\n",
      "372 [D loss: 0.317016]\n",
      "373 [D loss: 0.311113]\n",
      "374 [D loss: 0.321663]\n",
      "375 [D loss: 0.343585]\n",
      "376 [D loss: 0.303557]\n",
      "377 [D loss: 0.310218]\n",
      "378 [D loss: 0.310678]\n",
      "379 [D loss: 0.320331]\n",
      "380 [D loss: 0.319940]\n",
      "381 [D loss: 0.310742]\n",
      "382 [D loss: 0.301750]\n",
      "383 [D loss: 0.310539]\n",
      "384 [D loss: 0.295417]\n",
      "385 [D loss: 0.324169]\n",
      "386 [D loss: 0.315022]\n",
      "387 [D loss: 0.310636]\n",
      "388 [D loss: 0.310647]\n",
      "389 [D loss: 0.309126]\n",
      "390 [D loss: 0.302091]\n",
      "391 [D loss: 0.307940]\n",
      "392 [D loss: 0.304068]\n",
      "393 [D loss: 0.306330]\n",
      "394 [D loss: 0.305028]\n",
      "395 [D loss: 0.307292]\n",
      "396 [D loss: 0.313793]\n",
      "397 [D loss: 0.309069]\n",
      "398 [D loss: 0.307868]\n",
      "399 [D loss: 0.305928]\n",
      "400 [D loss: 0.308499]\n",
      "401 [D loss: 0.298262]\n",
      "402 [D loss: 0.306993]\n",
      "403 [D loss: 0.305869]\n",
      "404 [D loss: 0.300055]\n",
      "405 [D loss: 0.312012]\n",
      "406 [D loss: 0.302587]\n",
      "407 [D loss: 0.301419]\n",
      "408 [D loss: 0.297939]\n",
      "409 [D loss: 0.300928]\n",
      "410 [D loss: 0.307717]\n",
      "411 [D loss: 0.317050]\n",
      "412 [D loss: 0.300226]\n",
      "413 [D loss: 0.315152]\n",
      "414 [D loss: 0.314641]\n",
      "415 [D loss: 0.314942]\n",
      "416 [D loss: 0.292777]\n",
      "417 [D loss: 0.300439]\n",
      "418 [D loss: 0.291907]\n",
      "419 [D loss: 0.305404]\n",
      "420 [D loss: 0.311459]\n",
      "421 [D loss: 0.305335]\n",
      "422 [D loss: 0.379059]\n",
      "423 [D loss: 0.297839]\n",
      "424 [D loss: 0.298063]\n",
      "425 [D loss: 0.309926]\n",
      "426 [D loss: 0.302499]\n",
      "427 [D loss: 0.298808]\n",
      "428 [D loss: 0.298303]\n",
      "429 [D loss: 0.301571]\n",
      "430 [D loss: 0.294277]\n",
      "431 [D loss: 0.301037]\n",
      "432 [D loss: 0.309794]\n",
      "433 [D loss: 0.295569]\n",
      "434 [D loss: 0.297780]\n",
      "435 [D loss: 0.302363]\n",
      "436 [D loss: 0.301371]\n",
      "437 [D loss: 0.293017]\n",
      "438 [D loss: 0.318193]\n",
      "439 [D loss: 0.317012]\n",
      "440 [D loss: 0.293609]\n",
      "441 [D loss: 0.307206]\n",
      "442 [D loss: 0.315974]\n",
      "443 [D loss: 0.310043]\n",
      "444 [D loss: 0.307499]\n",
      "445 [D loss: 0.299450]\n",
      "446 [D loss: 0.313752]\n",
      "447 [D loss: 0.299494]\n",
      "448 [D loss: 0.294119]\n",
      "449 [D loss: 0.299649]\n",
      "450 [D loss: 0.297209]\n",
      "451 [D loss: 0.296252]\n",
      "452 [D loss: 0.304249]\n",
      "453 [D loss: 0.301908]\n",
      "454 [D loss: 0.292021]\n",
      "455 [D loss: 0.303991]\n",
      "456 [D loss: 0.291348]\n",
      "457 [D loss: 0.288253]\n",
      "458 [D loss: 0.292242]\n",
      "459 [D loss: 0.346472]\n",
      "460 [D loss: 0.287702]\n",
      "461 [D loss: 0.303502]\n",
      "462 [D loss: 0.294151]\n",
      "463 [D loss: 0.293637]\n",
      "464 [D loss: 0.294916]\n",
      "465 [D loss: 0.290724]\n",
      "466 [D loss: 0.290085]\n",
      "467 [D loss: 0.315219]\n",
      "468 [D loss: 0.288553]\n",
      "469 [D loss: 0.302602]\n",
      "470 [D loss: 0.290792]\n",
      "471 [D loss: 0.294812]\n",
      "472 [D loss: 0.298044]\n",
      "473 [D loss: 0.305657]\n",
      "474 [D loss: 0.298956]\n",
      "475 [D loss: 0.286874]\n",
      "476 [D loss: 0.294592]\n",
      "477 [D loss: 0.297284]\n",
      "478 [D loss: 0.292611]\n",
      "479 [D loss: 0.286017]\n",
      "480 [D loss: 0.290372]\n",
      "481 [D loss: 0.293153]\n",
      "482 [D loss: 0.305805]\n",
      "483 [D loss: 0.308578]\n",
      "484 [D loss: 0.295557]\n",
      "485 [D loss: 0.288620]\n",
      "486 [D loss: 0.296359]\n",
      "487 [D loss: 0.289022]\n",
      "488 [D loss: 0.291096]\n",
      "489 [D loss: 0.287920]\n",
      "490 [D loss: 0.305375]\n",
      "491 [D loss: 0.292404]\n",
      "492 [D loss: 0.286807]\n",
      "493 [D loss: 0.298825]\n",
      "494 [D loss: 0.289873]\n",
      "495 [D loss: 0.299267]\n",
      "496 [D loss: 0.296246]\n",
      "497 [D loss: 0.292963]\n",
      "498 [D loss: 0.285401]\n",
      "499 [D loss: 0.284262]\n",
      "500 [D loss: 0.292642]\n",
      "501 [D loss: 0.299542]\n",
      "502 [D loss: 0.305653]\n",
      "503 [D loss: 0.283732]\n",
      "504 [D loss: 0.282703]\n",
      "505 [D loss: 0.285194]\n",
      "506 [D loss: 0.290990]\n",
      "507 [D loss: 0.288246]\n",
      "508 [D loss: 0.293757]\n",
      "509 [D loss: 0.280675]\n",
      "510 [D loss: 0.294604]\n",
      "511 [D loss: 0.299717]\n",
      "512 [D loss: 0.295437]\n",
      "513 [D loss: 0.280078]\n",
      "514 [D loss: 0.282173]\n",
      "515 [D loss: 0.284832]\n",
      "516 [D loss: 0.282685]\n",
      "517 [D loss: 0.282824]\n",
      "518 [D loss: 0.278189]\n",
      "519 [D loss: 0.284809]\n",
      "520 [D loss: 0.282853]\n",
      "521 [D loss: 0.289953]\n",
      "522 [D loss: 0.292373]\n",
      "523 [D loss: 0.296936]\n",
      "524 [D loss: 0.289341]\n",
      "525 [D loss: 0.287855]\n",
      "526 [D loss: 0.286772]\n",
      "527 [D loss: 0.296617]\n",
      "528 [D loss: 0.276129]\n",
      "529 [D loss: 0.292383]\n",
      "530 [D loss: 0.287225]\n",
      "531 [D loss: 0.278751]\n",
      "532 [D loss: 0.285191]\n",
      "533 [D loss: 0.282442]\n",
      "534 [D loss: 0.289116]\n",
      "535 [D loss: 0.293650]\n",
      "536 [D loss: 0.287343]\n",
      "537 [D loss: 0.285007]\n",
      "538 [D loss: 0.287969]\n",
      "539 [D loss: 0.317771]\n",
      "540 [D loss: 0.294781]\n",
      "541 [D loss: 0.283936]\n",
      "542 [D loss: 0.293645]\n",
      "543 [D loss: 0.284697]\n",
      "544 [D loss: 0.293358]\n",
      "545 [D loss: 0.282275]\n",
      "546 [D loss: 0.281467]\n",
      "547 [D loss: 0.289884]\n",
      "548 [D loss: 0.280758]\n",
      "549 [D loss: 0.284051]\n",
      "550 [D loss: 0.288233]\n",
      "551 [D loss: 0.290607]\n",
      "552 [D loss: 0.281649]\n",
      "553 [D loss: 0.275435]\n",
      "554 [D loss: 0.274134]\n",
      "555 [D loss: 0.307270]\n",
      "556 [D loss: 0.290056]\n",
      "557 [D loss: 0.329263]\n",
      "558 [D loss: 0.282195]\n",
      "559 [D loss: 0.295652]\n",
      "560 [D loss: 0.281697]\n",
      "561 [D loss: 0.283284]\n",
      "562 [D loss: 0.284919]\n",
      "563 [D loss: 0.277639]\n",
      "564 [D loss: 0.283681]\n",
      "565 [D loss: 0.278417]\n",
      "566 [D loss: 0.281164]\n",
      "567 [D loss: 0.273071]\n",
      "568 [D loss: 0.279086]\n",
      "569 [D loss: 0.282324]\n",
      "570 [D loss: 0.277049]\n",
      "571 [D loss: 0.276823]\n",
      "572 [D loss: 0.270058]\n",
      "573 [D loss: 0.284291]\n",
      "574 [D loss: 0.276277]\n",
      "575 [D loss: 0.267431]\n",
      "576 [D loss: 0.273428]\n",
      "577 [D loss: 0.281093]\n",
      "578 [D loss: 0.279507]\n",
      "579 [D loss: 0.283049]\n",
      "580 [D loss: 0.278167]\n",
      "581 [D loss: 0.267813]\n",
      "582 [D loss: 0.280721]\n",
      "583 [D loss: 0.283832]\n",
      "584 [D loss: 0.292997]\n",
      "585 [D loss: 0.274852]\n",
      "586 [D loss: 0.271049]\n",
      "587 [D loss: 0.283508]\n",
      "588 [D loss: 0.275967]\n",
      "589 [D loss: 0.273991]\n",
      "590 [D loss: 0.270805]\n",
      "591 [D loss: 0.291057]\n",
      "592 [D loss: 0.272991]\n",
      "593 [D loss: 0.280348]\n",
      "594 [D loss: 0.274431]\n",
      "595 [D loss: 0.265311]\n",
      "596 [D loss: 0.276157]\n",
      "597 [D loss: 0.271891]\n",
      "598 [D loss: 0.325990]\n",
      "599 [D loss: 0.275857]\n",
      "600 [D loss: 0.273619]\n",
      "601 [D loss: 0.288983]\n",
      "602 [D loss: 0.274993]\n",
      "603 [D loss: 0.263952]\n",
      "604 [D loss: 0.277133]\n",
      "605 [D loss: 0.284040]\n",
      "606 [D loss: 0.291307]\n",
      "607 [D loss: 0.278938]\n",
      "608 [D loss: 0.265273]\n",
      "609 [D loss: 0.273969]\n",
      "610 [D loss: 0.264063]\n",
      "611 [D loss: 0.271901]\n",
      "612 [D loss: 0.274611]\n",
      "613 [D loss: 0.282091]\n",
      "614 [D loss: 0.269390]\n",
      "615 [D loss: 0.268246]\n",
      "616 [D loss: 0.279931]\n",
      "617 [D loss: 0.272838]\n",
      "618 [D loss: 0.266839]\n",
      "619 [D loss: 0.281909]\n",
      "620 [D loss: 0.268951]\n",
      "621 [D loss: 0.274197]\n",
      "622 [D loss: 0.275527]\n",
      "623 [D loss: 0.273227]\n",
      "624 [D loss: 0.270400]\n",
      "625 [D loss: 0.273011]\n",
      "626 [D loss: 0.271073]\n",
      "627 [D loss: 0.265515]\n",
      "628 [D loss: 0.270844]\n",
      "629 [D loss: 0.265372]\n",
      "630 [D loss: 0.282249]\n",
      "631 [D loss: 0.268956]\n",
      "632 [D loss: 0.265654]\n",
      "633 [D loss: 0.272573]\n",
      "634 [D loss: 0.279099]\n",
      "635 [D loss: 0.268671]\n",
      "636 [D loss: 0.267236]\n",
      "637 [D loss: 0.276857]\n",
      "638 [D loss: 0.277284]\n",
      "639 [D loss: 0.266328]\n",
      "640 [D loss: 0.268594]\n",
      "641 [D loss: 0.278516]\n",
      "642 [D loss: 0.266724]\n",
      "643 [D loss: 0.275398]\n",
      "644 [D loss: 0.262088]\n",
      "645 [D loss: 0.272894]\n",
      "646 [D loss: 0.266906]\n",
      "647 [D loss: 0.274945]\n",
      "648 [D loss: 0.272551]\n",
      "649 [D loss: 0.271770]\n",
      "650 [D loss: 0.270641]\n",
      "651 [D loss: 0.270163]\n",
      "652 [D loss: 0.266186]\n",
      "653 [D loss: 0.263732]\n",
      "654 [D loss: 0.273429]\n",
      "655 [D loss: 0.268208]\n",
      "656 [D loss: 0.273380]\n",
      "657 [D loss: 0.280806]\n",
      "658 [D loss: 0.259470]\n",
      "659 [D loss: 0.260421]\n",
      "660 [D loss: 0.266456]\n",
      "661 [D loss: 0.261239]\n",
      "662 [D loss: 0.260858]\n",
      "663 [D loss: 0.257756]\n",
      "664 [D loss: 0.264186]\n",
      "665 [D loss: 0.287937]\n",
      "666 [D loss: 0.363364]\n",
      "667 [D loss: 0.263031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "668 [D loss: 0.266248]\n",
      "669 [D loss: 0.270703]\n",
      "670 [D loss: 0.265600]\n",
      "671 [D loss: 0.272191]\n",
      "672 [D loss: 0.318701]\n",
      "673 [D loss: 0.262886]\n",
      "674 [D loss: 0.261353]\n",
      "675 [D loss: 0.265153]\n",
      "676 [D loss: 0.264942]\n",
      "677 [D loss: 0.266125]\n",
      "678 [D loss: 0.266397]\n",
      "679 [D loss: 0.262858]\n",
      "680 [D loss: 0.259541]\n",
      "681 [D loss: 0.270350]\n",
      "682 [D loss: 0.267079]\n",
      "683 [D loss: 0.267645]\n",
      "684 [D loss: 0.284718]\n",
      "685 [D loss: 0.252535]\n",
      "686 [D loss: 0.266327]\n",
      "687 [D loss: 0.266572]\n",
      "688 [D loss: 0.257200]\n",
      "689 [D loss: 0.275976]\n",
      "690 [D loss: 0.266376]\n",
      "691 [D loss: 0.269848]\n",
      "692 [D loss: 0.259641]\n",
      "693 [D loss: 0.265617]\n",
      "694 [D loss: 0.263667]\n",
      "695 [D loss: 0.287400]\n",
      "696 [D loss: 0.266186]\n",
      "697 [D loss: 0.272100]\n",
      "698 [D loss: 0.265068]\n",
      "699 [D loss: 0.268523]\n",
      "700 [D loss: 0.258919]\n",
      "701 [D loss: 0.253631]\n",
      "702 [D loss: 0.267419]\n",
      "703 [D loss: 0.270785]\n",
      "704 [D loss: 0.262081]\n",
      "705 [D loss: 0.262285]\n",
      "706 [D loss: 0.278156]\n",
      "707 [D loss: 0.250528]\n",
      "708 [D loss: 0.259005]\n",
      "709 [D loss: 0.265866]\n",
      "710 [D loss: 0.263519]\n",
      "711 [D loss: 0.258637]\n",
      "712 [D loss: 0.265959]\n",
      "713 [D loss: 0.262801]\n",
      "714 [D loss: 0.256827]\n",
      "715 [D loss: 0.263740]\n",
      "716 [D loss: 0.257062]\n",
      "717 [D loss: 0.263341]\n",
      "718 [D loss: 0.257971]\n",
      "719 [D loss: 0.265009]\n",
      "720 [D loss: 0.251752]\n",
      "721 [D loss: 0.261861]\n",
      "722 [D loss: 0.255710]\n",
      "723 [D loss: 0.257879]\n",
      "724 [D loss: 0.254021]\n",
      "725 [D loss: 0.260540]\n",
      "726 [D loss: 0.269453]\n",
      "727 [D loss: 0.255957]\n",
      "728 [D loss: 0.255269]\n",
      "729 [D loss: 0.272440]\n",
      "730 [D loss: 0.261589]\n",
      "731 [D loss: 0.269536]\n",
      "732 [D loss: 0.253107]\n",
      "733 [D loss: 0.257501]\n",
      "734 [D loss: 0.266822]\n",
      "735 [D loss: 0.261036]\n",
      "736 [D loss: 0.257356]\n",
      "737 [D loss: 0.260223]\n",
      "738 [D loss: 0.266257]\n",
      "739 [D loss: 0.255190]\n",
      "740 [D loss: 0.250368]\n",
      "741 [D loss: 0.259384]\n",
      "742 [D loss: 0.259621]\n",
      "743 [D loss: 0.252934]\n",
      "744 [D loss: 0.256237]\n",
      "745 [D loss: 0.249537]\n",
      "746 [D loss: 0.264532]\n",
      "747 [D loss: 0.257332]\n",
      "748 [D loss: 0.255741]\n",
      "749 [D loss: 0.258424]\n",
      "750 [D loss: 0.276322]\n",
      "751 [D loss: 0.259980]\n",
      "752 [D loss: 0.252265]\n",
      "753 [D loss: 0.252296]\n",
      "754 [D loss: 0.255214]\n",
      "755 [D loss: 0.252735]\n",
      "756 [D loss: 0.253158]\n",
      "757 [D loss: 0.294735]\n",
      "758 [D loss: 0.292492]\n",
      "759 [D loss: 0.245142]\n",
      "760 [D loss: 0.258588]\n",
      "761 [D loss: 0.268110]\n",
      "762 [D loss: 0.251005]\n",
      "763 [D loss: 0.350231]\n",
      "764 [D loss: 0.247014]\n",
      "765 [D loss: 0.259761]\n",
      "766 [D loss: 0.252385]\n",
      "767 [D loss: 0.247899]\n",
      "768 [D loss: 0.253975]\n",
      "769 [D loss: 0.249038]\n",
      "770 [D loss: 0.259550]\n",
      "771 [D loss: 0.262750]\n",
      "772 [D loss: 0.250955]\n",
      "773 [D loss: 0.247571]\n",
      "774 [D loss: 0.248761]\n",
      "775 [D loss: 0.253690]\n",
      "776 [D loss: 0.258730]\n",
      "777 [D loss: 0.243140]\n",
      "778 [D loss: 0.264512]\n",
      "779 [D loss: 0.256236]\n",
      "780 [D loss: 0.255901]\n",
      "781 [D loss: 0.242777]\n",
      "782 [D loss: 0.249863]\n",
      "783 [D loss: 0.256874]\n",
      "784 [D loss: 0.249524]\n",
      "785 [D loss: 0.250761]\n",
      "786 [D loss: 0.242059]\n",
      "787 [D loss: 0.262303]\n",
      "788 [D loss: 0.254628]\n",
      "789 [D loss: 0.250975]\n",
      "790 [D loss: 0.247910]\n",
      "791 [D loss: 0.251108]\n",
      "792 [D loss: 0.264443]\n",
      "793 [D loss: 0.255028]\n",
      "794 [D loss: 0.251226]\n",
      "795 [D loss: 0.249668]\n",
      "796 [D loss: 0.266171]\n",
      "797 [D loss: 0.260599]\n",
      "798 [D loss: 0.261812]\n",
      "799 [D loss: 0.249037]\n",
      "800 [D loss: 0.246140]\n",
      "801 [D loss: 0.246944]\n",
      "802 [D loss: 0.253003]\n",
      "803 [D loss: 0.234773]\n",
      "804 [D loss: 0.250155]\n",
      "805 [D loss: 0.265352]\n",
      "806 [D loss: 0.244297]\n",
      "807 [D loss: 0.250273]\n",
      "808 [D loss: 0.245414]\n",
      "809 [D loss: 0.253282]\n",
      "810 [D loss: 0.237204]\n",
      "811 [D loss: 0.240742]\n",
      "812 [D loss: 0.245531]\n",
      "813 [D loss: 0.256460]\n",
      "814 [D loss: 0.246138]\n",
      "815 [D loss: 0.243384]\n",
      "816 [D loss: 0.248160]\n",
      "817 [D loss: 0.255481]\n",
      "818 [D loss: 0.245482]\n",
      "819 [D loss: 0.249897]\n",
      "820 [D loss: 0.260802]\n",
      "821 [D loss: 0.240129]\n",
      "822 [D loss: 0.242257]\n",
      "823 [D loss: 0.246589]\n",
      "824 [D loss: 0.244810]\n",
      "825 [D loss: 0.241063]\n",
      "826 [D loss: 0.248772]\n",
      "827 [D loss: 0.241372]\n",
      "828 [D loss: 0.255845]\n",
      "829 [D loss: 0.248441]\n",
      "830 [D loss: 0.253526]\n",
      "831 [D loss: 0.243666]\n",
      "832 [D loss: 0.240460]\n",
      "833 [D loss: 0.240418]\n",
      "834 [D loss: 0.245825]\n",
      "835 [D loss: 0.251311]\n",
      "836 [D loss: 0.239348]\n",
      "837 [D loss: 0.237033]\n",
      "838 [D loss: 0.243075]\n",
      "839 [D loss: 0.239308]\n",
      "840 [D loss: 0.240033]\n",
      "841 [D loss: 0.238504]\n",
      "842 [D loss: 0.234797]\n",
      "843 [D loss: 0.241426]\n",
      "844 [D loss: 0.243324]\n",
      "845 [D loss: 0.239031]\n",
      "846 [D loss: 0.242320]\n",
      "847 [D loss: 0.235616]\n",
      "848 [D loss: 0.240187]\n",
      "849 [D loss: 0.246599]\n",
      "850 [D loss: 0.237291]\n",
      "851 [D loss: 0.240981]\n",
      "852 [D loss: 0.234800]\n",
      "853 [D loss: 0.230832]\n",
      "854 [D loss: 0.243637]\n",
      "855 [D loss: 0.231352]\n",
      "856 [D loss: 0.234055]\n",
      "857 [D loss: 0.241784]\n",
      "858 [D loss: 0.241839]\n",
      "859 [D loss: 0.234060]\n",
      "860 [D loss: 0.236974]\n",
      "861 [D loss: 0.242140]\n",
      "862 [D loss: 0.239585]\n",
      "863 [D loss: 0.235738]\n",
      "864 [D loss: 0.256065]\n",
      "865 [D loss: 0.235607]\n",
      "866 [D loss: 0.247866]\n",
      "867 [D loss: 0.239776]\n",
      "868 [D loss: 0.249639]\n",
      "869 [D loss: 0.232954]\n",
      "870 [D loss: 0.232414]\n",
      "871 [D loss: 0.245305]\n",
      "872 [D loss: 0.244580]\n",
      "873 [D loss: 0.256021]\n",
      "874 [D loss: 0.247328]\n",
      "875 [D loss: 0.238354]\n",
      "876 [D loss: 0.242087]\n",
      "877 [D loss: 0.235528]\n",
      "878 [D loss: 0.237780]\n",
      "879 [D loss: 0.240503]\n",
      "880 [D loss: 0.228425]\n",
      "881 [D loss: 0.239984]\n",
      "882 [D loss: 0.230665]\n",
      "883 [D loss: 0.228242]\n",
      "884 [D loss: 0.247029]\n",
      "885 [D loss: 0.230551]\n",
      "886 [D loss: 0.232642]\n",
      "887 [D loss: 0.231737]\n",
      "888 [D loss: 0.237915]\n",
      "889 [D loss: 0.230604]\n",
      "890 [D loss: 0.238406]\n",
      "891 [D loss: 0.237362]\n",
      "892 [D loss: 0.235949]\n",
      "893 [D loss: 0.238440]\n",
      "894 [D loss: 0.225447]\n",
      "895 [D loss: 0.242895]\n",
      "896 [D loss: 0.239659]\n",
      "897 [D loss: 0.227091]\n",
      "898 [D loss: 0.231593]\n",
      "899 [D loss: 0.233369]\n",
      "900 [D loss: 0.242083]\n",
      "901 [D loss: 0.234055]\n",
      "902 [D loss: 0.262428]\n",
      "903 [D loss: 0.224103]\n",
      "904 [D loss: 0.227685]\n",
      "905 [D loss: 0.229624]\n",
      "906 [D loss: 0.230985]\n",
      "907 [D loss: 0.233349]\n",
      "908 [D loss: 0.231418]\n",
      "909 [D loss: 0.229486]\n",
      "910 [D loss: 0.230601]\n",
      "911 [D loss: 0.232191]\n",
      "912 [D loss: 0.221655]\n",
      "913 [D loss: 0.239595]\n",
      "914 [D loss: 0.232850]\n",
      "915 [D loss: 0.229483]\n",
      "916 [D loss: 0.238850]\n",
      "917 [D loss: 0.237873]\n",
      "918 [D loss: 0.227699]\n",
      "919 [D loss: 0.234671]\n",
      "920 [D loss: 0.232786]\n",
      "921 [D loss: 0.232238]\n",
      "922 [D loss: 0.222315]\n",
      "923 [D loss: 0.226656]\n",
      "924 [D loss: 0.232070]\n",
      "925 [D loss: 0.236162]\n",
      "926 [D loss: 0.224287]\n",
      "927 [D loss: 0.230895]\n",
      "928 [D loss: 0.238494]\n",
      "929 [D loss: 0.244233]\n",
      "930 [D loss: 0.232285]\n",
      "931 [D loss: 0.237837]\n",
      "932 [D loss: 0.233023]\n",
      "933 [D loss: 0.225976]\n",
      "934 [D loss: 0.225381]\n",
      "935 [D loss: 0.231828]\n",
      "936 [D loss: 0.225266]\n",
      "937 [D loss: 0.223646]\n",
      "938 [D loss: 0.228589]\n",
      "939 [D loss: 0.233324]\n",
      "940 [D loss: 0.226637]\n",
      "941 [D loss: 0.237090]\n",
      "942 [D loss: 0.216809]\n",
      "943 [D loss: 0.238930]\n",
      "944 [D loss: 0.229334]\n",
      "945 [D loss: 0.225132]\n",
      "946 [D loss: 0.224783]\n",
      "947 [D loss: 0.231758]\n",
      "948 [D loss: 0.226222]\n",
      "949 [D loss: 0.239797]\n",
      "950 [D loss: 0.232502]\n",
      "951 [D loss: 0.224717]\n",
      "952 [D loss: 0.224957]\n",
      "953 [D loss: 0.220372]\n",
      "954 [D loss: 0.227788]\n",
      "955 [D loss: 0.222811]\n",
      "956 [D loss: 0.233065]\n",
      "957 [D loss: 0.216075]\n",
      "958 [D loss: 0.224979]\n",
      "959 [D loss: 0.228293]\n",
      "960 [D loss: 0.222174]\n",
      "961 [D loss: 0.234245]\n",
      "962 [D loss: 0.220505]\n",
      "963 [D loss: 0.242343]\n",
      "964 [D loss: 0.231276]\n",
      "965 [D loss: 0.215809]\n",
      "966 [D loss: 0.218093]\n",
      "967 [D loss: 0.221190]\n",
      "968 [D loss: 0.237663]\n",
      "969 [D loss: 0.224051]\n",
      "970 [D loss: 0.222766]\n",
      "971 [D loss: 0.235152]\n",
      "972 [D loss: 0.217843]\n",
      "973 [D loss: 0.226987]\n",
      "974 [D loss: 0.230308]\n",
      "975 [D loss: 0.217713]\n",
      "976 [D loss: 0.226648]\n",
      "977 [D loss: 0.232290]\n",
      "978 [D loss: 0.228907]\n",
      "979 [D loss: 0.218554]\n",
      "980 [D loss: 0.217252]\n",
      "981 [D loss: 0.221814]\n",
      "982 [D loss: 0.220967]\n",
      "983 [D loss: 0.227131]\n"
     ]
    }
   ],
   "source": [
    "class Autoencoder():\n",
    "    \"\"\"An Autoencoder with deep fully-connected neural nets.\n",
    "\n",
    "    Training Data: MNIST Handwritten Digits (28x28 images)\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.img_rows = 28\n",
    "        self.img_cols = 28\n",
    "        self.img_dim = self.img_rows * self.img_cols\n",
    "        self.latent_dim = 128 # The dimension of the data embedding\n",
    "\n",
    "        optimizer = Adam(learning_rate=0.0002, b1=0.5)\n",
    "        loss_function = SquareLoss\n",
    "\n",
    "        self.encoder = self.build_encoder(optimizer, loss_function)\n",
    "        self.decoder = self.build_decoder(optimizer, loss_function)\n",
    "\n",
    "        self.autoencoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        self.autoencoder.layers.extend(self.encoder.layers)\n",
    "        self.autoencoder.layers.extend(self.decoder.layers)\n",
    "\n",
    "        print ()\n",
    "        self.autoencoder.summary(name=\"Variational Autoencoder\")\n",
    "\n",
    "    def build_encoder(self, optimizer, loss_function):\n",
    "\n",
    "        encoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        encoder.add(Dense(512, input_shape=(self.img_dim,)))\n",
    "        encoder.add(Activation('leaky_relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(256))\n",
    "        encoder.add(Activation('leaky_relu'))\n",
    "        encoder.add(BatchNormalization(momentum=0.8))\n",
    "        encoder.add(Dense(self.latent_dim))\n",
    "\n",
    "        return encoder\n",
    "\n",
    "    def build_decoder(self, optimizer, loss_function):\n",
    "\n",
    "        decoder = NeuralNetwork(optimizer=optimizer, loss=loss_function)\n",
    "        decoder.add(Dense(256, input_shape=(self.latent_dim,)))\n",
    "        decoder.add(Activation('leaky_relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(512))\n",
    "        decoder.add(Activation('leaky_relu'))\n",
    "        decoder.add(BatchNormalization(momentum=0.8))\n",
    "        decoder.add(Dense(self.img_dim))\n",
    "        decoder.add(Activation('tanh'))\n",
    "\n",
    "        return decoder\n",
    "\n",
    "    def train(self, n_epochs, batch_size=128, save_interval=50):\n",
    "\n",
    "        mnist = datasets.fetch_mldata('MNIST original')\n",
    "\n",
    "        X = mnist.data\n",
    "        y = mnist.target\n",
    "\n",
    "        # Rescale [-1, 1]\n",
    "        X = (X.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X.shape[0], batch_size)\n",
    "            imgs = X[idx]\n",
    "\n",
    "            # Train the Autoencoder\n",
    "            loss, _ = self.autoencoder.train_on_batch(imgs, imgs)\n",
    "\n",
    "            # Display the progress\n",
    "            print (\"%d [D loss: %f]\" % (epoch, loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch, X)\n",
    "\n",
    "    def save_imgs(self, epoch, X):\n",
    "        r, c = 5, 5 # Grid size\n",
    "        # Select a random half batch of images\n",
    "        idx = np.random.randint(0, X.shape[0], r*c)\n",
    "        imgs = X[idx]\n",
    "        # Generate images and reshape to image shape\n",
    "        gen_imgs = self.autoencoder.predict(imgs).reshape((-1, self.img_rows, self.img_cols))\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        plt.suptitle(\"Autoencoder\")\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i,j].imshow(gen_imgs[cnt,:,:], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"ae_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    ae = Autoencoder()\n",
    "    ae.train(n_epochs=200000, batch_size=64, save_interval=400)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
