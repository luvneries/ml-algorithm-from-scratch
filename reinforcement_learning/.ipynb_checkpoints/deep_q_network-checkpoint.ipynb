{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make this notebook compatible for Python 2 and 3\n",
    "from __future__ import division, print_function\n",
    "\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import math\n",
    "import progressbar\n",
    "import gym\n",
    "import random\n",
    "\n",
    "# for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# to import module from parent directory\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "# Dataset API from sklearn\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import helper functions\n",
    "from utils import mean_squared_error, train_test_split, Plot\n",
    "from utils import standardize, to_categorical, accuracy_score\n",
    "from utils.misc import bar_widgets\n",
    "\n",
    "from deep_learning.loss_functions import SquareLoss, CrossEntropy\n",
    "from deep_learning.optimizers import Adam\n",
    "from deep_learning.layers import Dense, Activation\n",
    "from deep_learning.neural_network import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepQNetwork():\n",
    "    \"\"\"Q-Learning with deep neural network to learn the control policy. \n",
    "    Uses a deep neural network model to predict the expected utility (Q-value) of executing an action in a given state. \n",
    "\n",
    "    Reference: https://arxiv.org/abs/1312.5602\n",
    "    Parameters:\n",
    "    -----------\n",
    "    env_name: string\n",
    "        The environment that the agent will explore. \n",
    "        Check: https://gym.openai.com/envs\n",
    "    epsilon: float\n",
    "        The epsilon-greedy value. The probability that the agent should select a random action instead of\n",
    "        the action that will maximize the expected utility. \n",
    "    gamma: float\n",
    "        Determines how much the agent should consider future rewards. \n",
    "    decay_rate: float\n",
    "        The rate of decay for the epsilon value after each epoch.\n",
    "    min_epsilon: float\n",
    "        The value which epsilon will approach as the training progresses.\n",
    "    \"\"\"\n",
    "    def __init__(self, env_name='CartPole-v1', epsilon=1, gamma=0.9, decay_rate=0.005, min_epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.decay_rate = decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.memory_size = 300\n",
    "        self.memory = []\n",
    "\n",
    "        # Initialize the environment\n",
    "        self.env = gym.make(env_name)\n",
    "        self.n_states = self.env.observation_space.shape[0]\n",
    "        self.n_actions = self.env.action_space.n\n",
    "    \n",
    "    def set_model(self, model):\n",
    "        self.model = model(n_inputs=self.n_states, n_outputs=self.n_actions)\n",
    "\n",
    "    def _select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            # Choose action randomly\n",
    "            action = np.random.randint(self.n_actions)\n",
    "        else:\n",
    "            # Take action with highest predicted utility given state\n",
    "            action = np.argmax(self.model.predict(state), axis=1)[0]\n",
    "\n",
    "        return action\n",
    "\n",
    "    def _memorize(self, state, action, reward, new_state, done):\n",
    "        self.memory.append((state, action, reward, new_state, done))\n",
    "        # Make sure we restrict memory size to specified limit\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "\n",
    "    def _construct_training_set(self, replay):\n",
    "        # Select states and new states from replay\n",
    "        states = np.array([a[0] for a in replay])\n",
    "        new_states = np.array([a[3] for a in replay])\n",
    "\n",
    "        # Predict the expected utility of current state and new state\n",
    "        Q = self.model.predict(states)\n",
    "        Q_new = self.model.predict(new_states)\n",
    "\n",
    "        replay_size = len(replay)\n",
    "        X = np.empty((replay_size, self.n_states))\n",
    "        y = np.empty((replay_size, self.n_actions))\n",
    "        \n",
    "        # Construct training set\n",
    "        for i in range(replay_size):\n",
    "            state_r, action_r, reward_r, new_state_r, done_r = replay[i]\n",
    "\n",
    "            target = Q[i]\n",
    "            target[action_r] = reward_r\n",
    "            # If we're done the utility is simply the reward of executing action a in\n",
    "            # state s, otherwise we add the expected maximum future reward as well\n",
    "            if not done_r:\n",
    "                target[action_r] += self.gamma * np.amax(Q_new[i])\n",
    "\n",
    "            X[i] = state_r\n",
    "            y[i] = target\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def train(self, n_epochs=500, batch_size=32):\n",
    "        max_reward = 0\n",
    "\n",
    "        for epoch in range(n_epochs):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "\n",
    "            epoch_loss = []\n",
    "            while True:\n",
    "\n",
    "                action = self._select_action(state)\n",
    "                # Take a step\n",
    "                new_state, reward, done, _ = self.env.step(action)\n",
    "\n",
    "                self._memorize(state, action, reward, new_state, done)\n",
    "\n",
    "                # Sample replay batch from memory\n",
    "                _batch_size = min(len(self.memory), batch_size)\n",
    "                replay = random.sample(self.memory, _batch_size)\n",
    "\n",
    "                # Construct training set from replay\n",
    "                X, y = self._construct_training_set(replay)\n",
    "\n",
    "                # Learn control policy\n",
    "                loss = self.model.train_on_batch(X, y)\n",
    "                epoch_loss.append(loss)\n",
    "\n",
    "                total_reward += reward\n",
    "                state = new_state\n",
    "\n",
    "                if done: break\n",
    "            \n",
    "            epoch_loss = np.mean(epoch_loss)\n",
    "\n",
    "            # Reduce the epsilon parameter\n",
    "            self.epsilon = self.min_epsilon + (1.0 - self.min_epsilon) * np.exp(-self.decay_rate * epoch)\n",
    "            \n",
    "            max_reward = max(max_reward, total_reward)\n",
    "\n",
    "            print (\"%d [Loss: %.4f, Reward: %s, Epsilon: %.4f, Max Reward: %s]\" % (epoch, epoch_loss, total_reward, self.epsilon, max_reward))\n",
    "\n",
    "        print (\"Training Finished\")\n",
    "\n",
    "    def play(self, n_epochs):\n",
    "        # self.env = gym.wrappers.Monitor(self.env, '/tmp/cartpole-experiment-1', force=True)\n",
    "        for epoch in range(n_epochs):\n",
    "            state = self.env.reset()\n",
    "            total_reward = 0\n",
    "            while True:\n",
    "                self.env.render()\n",
    "                action = np.argmax(self.model.predict(state), axis=1)[0]\n",
    "                state, reward, done, _ = self.env.step(action)\n",
    "                total_reward += reward\n",
    "                if done: break\n",
    "            print (\"%d Reward: %s\" % (epoch, total_reward))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "+----------------+\n",
      "| Deep Q-Network |\n",
      "+----------------+\n",
      "Input Shape: (4,)\n",
      "+-------------------+------------+--------------+\n",
      "| Layer Type        | Parameters | Output Shape |\n",
      "+-------------------+------------+--------------+\n",
      "| Dense             | 320        | (64,)        |\n",
      "| Activation (ReLU) | 0          | (64,)        |\n",
      "| Dense             | 130        | (2,)         |\n",
      "+-------------------+------------+--------------+\n",
      "Total Parameters: 450\n",
      "\n",
      "0 [Loss: 0.1209, Reward: 37.0, Epsilon: 1.0000, Max Reward: 37.0]\n",
      "1 [Loss: 0.1102, Reward: 9.0, Epsilon: 0.9955, Max Reward: 37.0]\n",
      "2 [Loss: 0.1045, Reward: 29.0, Epsilon: 0.9910, Max Reward: 37.0]\n",
      "3 [Loss: 0.1043, Reward: 19.0, Epsilon: 0.9866, Max Reward: 37.0]\n",
      "4 [Loss: 0.1273, Reward: 12.0, Epsilon: 0.9822, Max Reward: 37.0]\n",
      "5 [Loss: 0.1235, Reward: 27.0, Epsilon: 0.9778, Max Reward: 37.0]\n",
      "6 [Loss: 0.1093, Reward: 20.0, Epsilon: 0.9734, Max Reward: 37.0]\n",
      "7 [Loss: 0.1192, Reward: 61.0, Epsilon: 0.9690, Max Reward: 61.0]\n",
      "8 [Loss: 0.1038, Reward: 13.0, Epsilon: 0.9647, Max Reward: 61.0]\n",
      "9 [Loss: 0.1652, Reward: 18.0, Epsilon: 0.9604, Max Reward: 61.0]\n",
      "10 [Loss: 0.1293, Reward: 17.0, Epsilon: 0.9561, Max Reward: 61.0]\n",
      "11 [Loss: 0.1644, Reward: 14.0, Epsilon: 0.9518, Max Reward: 61.0]\n",
      "12 [Loss: 0.1031, Reward: 57.0, Epsilon: 0.9476, Max Reward: 61.0]\n",
      "13 [Loss: 0.1102, Reward: 13.0, Epsilon: 0.9434, Max Reward: 61.0]\n",
      "14 [Loss: 0.0829, Reward: 57.0, Epsilon: 0.9392, Max Reward: 61.0]\n",
      "15 [Loss: 0.0758, Reward: 15.0, Epsilon: 0.9350, Max Reward: 61.0]\n",
      "16 [Loss: 0.0913, Reward: 12.0, Epsilon: 0.9308, Max Reward: 61.0]\n",
      "17 [Loss: 0.0702, Reward: 18.0, Epsilon: 0.9267, Max Reward: 61.0]\n",
      "18 [Loss: 0.0914, Reward: 14.0, Epsilon: 0.9225, Max Reward: 61.0]\n",
      "19 [Loss: 0.1094, Reward: 18.0, Epsilon: 0.9184, Max Reward: 61.0]\n",
      "20 [Loss: 0.1368, Reward: 14.0, Epsilon: 0.9144, Max Reward: 61.0]\n",
      "21 [Loss: 0.0969, Reward: 13.0, Epsilon: 0.9103, Max Reward: 61.0]\n",
      "22 [Loss: 0.1023, Reward: 13.0, Epsilon: 0.9063, Max Reward: 61.0]\n",
      "23 [Loss: 0.0863, Reward: 43.0, Epsilon: 0.9022, Max Reward: 61.0]\n",
      "24 [Loss: 0.0747, Reward: 14.0, Epsilon: 0.8982, Max Reward: 61.0]\n",
      "25 [Loss: 0.0825, Reward: 13.0, Epsilon: 0.8942, Max Reward: 61.0]\n",
      "26 [Loss: 0.0930, Reward: 18.0, Epsilon: 0.8903, Max Reward: 61.0]\n",
      "27 [Loss: 0.0993, Reward: 14.0, Epsilon: 0.8863, Max Reward: 61.0]\n",
      "28 [Loss: 0.0705, Reward: 31.0, Epsilon: 0.8824, Max Reward: 61.0]\n",
      "29 [Loss: 0.0986, Reward: 22.0, Epsilon: 0.8785, Max Reward: 61.0]\n",
      "30 [Loss: 0.0856, Reward: 18.0, Epsilon: 0.8746, Max Reward: 61.0]\n",
      "31 [Loss: 0.1021, Reward: 12.0, Epsilon: 0.8708, Max Reward: 61.0]\n",
      "32 [Loss: 0.0761, Reward: 23.0, Epsilon: 0.8669, Max Reward: 61.0]\n",
      "33 [Loss: 0.0975, Reward: 24.0, Epsilon: 0.8631, Max Reward: 61.0]\n",
      "34 [Loss: 0.0848, Reward: 9.0, Epsilon: 0.8593, Max Reward: 61.0]\n",
      "35 [Loss: 0.0922, Reward: 12.0, Epsilon: 0.8555, Max Reward: 61.0]\n",
      "36 [Loss: 0.0856, Reward: 21.0, Epsilon: 0.8517, Max Reward: 61.0]\n",
      "37 [Loss: 0.0780, Reward: 14.0, Epsilon: 0.8480, Max Reward: 61.0]\n",
      "38 [Loss: 0.0761, Reward: 25.0, Epsilon: 0.8443, Max Reward: 61.0]\n",
      "39 [Loss: 0.0428, Reward: 8.0, Epsilon: 0.8406, Max Reward: 61.0]\n",
      "40 [Loss: 0.0642, Reward: 21.0, Epsilon: 0.8369, Max Reward: 61.0]\n",
      "41 [Loss: 0.0730, Reward: 16.0, Epsilon: 0.8332, Max Reward: 61.0]\n",
      "42 [Loss: 0.0716, Reward: 19.0, Epsilon: 0.8295, Max Reward: 61.0]\n",
      "43 [Loss: 0.0794, Reward: 13.0, Epsilon: 0.8259, Max Reward: 61.0]\n",
      "44 [Loss: 0.0660, Reward: 30.0, Epsilon: 0.8223, Max Reward: 61.0]\n",
      "45 [Loss: 0.0579, Reward: 12.0, Epsilon: 0.8187, Max Reward: 61.0]\n",
      "46 [Loss: 0.0707, Reward: 13.0, Epsilon: 0.8151, Max Reward: 61.0]\n",
      "47 [Loss: 0.0689, Reward: 15.0, Epsilon: 0.8115, Max Reward: 61.0]\n",
      "48 [Loss: 0.0717, Reward: 25.0, Epsilon: 0.8080, Max Reward: 61.0]\n",
      "49 [Loss: 0.0501, Reward: 20.0, Epsilon: 0.8044, Max Reward: 61.0]\n",
      "50 [Loss: 0.0687, Reward: 38.0, Epsilon: 0.8009, Max Reward: 61.0]\n",
      "51 [Loss: 0.0544, Reward: 10.0, Epsilon: 0.7974, Max Reward: 61.0]\n",
      "52 [Loss: 0.0703, Reward: 12.0, Epsilon: 0.7939, Max Reward: 61.0]\n",
      "53 [Loss: 0.0406, Reward: 12.0, Epsilon: 0.7905, Max Reward: 61.0]\n",
      "54 [Loss: 0.0620, Reward: 30.0, Epsilon: 0.7870, Max Reward: 61.0]\n",
      "55 [Loss: 0.0708, Reward: 14.0, Epsilon: 0.7836, Max Reward: 61.0]\n",
      "56 [Loss: 0.0707, Reward: 17.0, Epsilon: 0.7802, Max Reward: 61.0]\n",
      "57 [Loss: 0.0445, Reward: 15.0, Epsilon: 0.7768, Max Reward: 61.0]\n",
      "58 [Loss: 0.0507, Reward: 16.0, Epsilon: 0.7734, Max Reward: 61.0]\n",
      "59 [Loss: 0.0439, Reward: 10.0, Epsilon: 0.7701, Max Reward: 61.0]\n",
      "60 [Loss: 0.0491, Reward: 24.0, Epsilon: 0.7667, Max Reward: 61.0]\n",
      "61 [Loss: 0.0566, Reward: 13.0, Epsilon: 0.7634, Max Reward: 61.0]\n",
      "62 [Loss: 0.0443, Reward: 15.0, Epsilon: 0.7601, Max Reward: 61.0]\n",
      "63 [Loss: 0.0527, Reward: 11.0, Epsilon: 0.7568, Max Reward: 61.0]\n",
      "64 [Loss: 0.0490, Reward: 18.0, Epsilon: 0.7535, Max Reward: 61.0]\n",
      "65 [Loss: 0.0548, Reward: 23.0, Epsilon: 0.7503, Max Reward: 61.0]\n",
      "66 [Loss: 0.0374, Reward: 17.0, Epsilon: 0.7470, Max Reward: 61.0]\n",
      "67 [Loss: 0.0619, Reward: 8.0, Epsilon: 0.7438, Max Reward: 61.0]\n",
      "68 [Loss: 0.0607, Reward: 20.0, Epsilon: 0.7406, Max Reward: 61.0]\n",
      "69 [Loss: 0.0473, Reward: 20.0, Epsilon: 0.7374, Max Reward: 61.0]\n",
      "70 [Loss: 0.0519, Reward: 9.0, Epsilon: 0.7342, Max Reward: 61.0]\n",
      "71 [Loss: 0.0417, Reward: 19.0, Epsilon: 0.7311, Max Reward: 61.0]\n",
      "72 [Loss: 0.0571, Reward: 10.0, Epsilon: 0.7279, Max Reward: 61.0]\n",
      "73 [Loss: 0.0514, Reward: 35.0, Epsilon: 0.7248, Max Reward: 61.0]\n",
      "74 [Loss: 0.0508, Reward: 17.0, Epsilon: 0.7217, Max Reward: 61.0]\n",
      "75 [Loss: 0.0327, Reward: 13.0, Epsilon: 0.7186, Max Reward: 61.0]\n",
      "76 [Loss: 0.0439, Reward: 16.0, Epsilon: 0.7155, Max Reward: 61.0]\n",
      "77 [Loss: 0.0549, Reward: 14.0, Epsilon: 0.7124, Max Reward: 61.0]\n",
      "78 [Loss: 0.0393, Reward: 30.0, Epsilon: 0.7094, Max Reward: 61.0]\n",
      "79 [Loss: 0.0430, Reward: 19.0, Epsilon: 0.7063, Max Reward: 61.0]\n",
      "80 [Loss: 0.0387, Reward: 33.0, Epsilon: 0.7033, Max Reward: 61.0]\n",
      "81 [Loss: 0.0475, Reward: 29.0, Epsilon: 0.7003, Max Reward: 61.0]\n",
      "82 [Loss: 0.0390, Reward: 67.0, Epsilon: 0.6973, Max Reward: 67.0]\n",
      "83 [Loss: 0.0327, Reward: 32.0, Epsilon: 0.6943, Max Reward: 67.0]\n",
      "84 [Loss: 0.0435, Reward: 30.0, Epsilon: 0.6913, Max Reward: 67.0]\n",
      "85 [Loss: 0.0290, Reward: 31.0, Epsilon: 0.6884, Max Reward: 67.0]\n",
      "86 [Loss: 0.0345, Reward: 14.0, Epsilon: 0.6855, Max Reward: 67.0]\n",
      "87 [Loss: 0.0399, Reward: 30.0, Epsilon: 0.6825, Max Reward: 67.0]\n",
      "88 [Loss: 0.0390, Reward: 23.0, Epsilon: 0.6796, Max Reward: 67.0]\n",
      "89 [Loss: 0.0336, Reward: 32.0, Epsilon: 0.6767, Max Reward: 67.0]\n",
      "90 [Loss: 0.0350, Reward: 20.0, Epsilon: 0.6739, Max Reward: 67.0]\n",
      "91 [Loss: 0.0305, Reward: 28.0, Epsilon: 0.6710, Max Reward: 67.0]\n",
      "92 [Loss: 0.0437, Reward: 19.0, Epsilon: 0.6682, Max Reward: 67.0]\n",
      "93 [Loss: 0.0370, Reward: 14.0, Epsilon: 0.6653, Max Reward: 67.0]\n",
      "94 [Loss: 0.0302, Reward: 20.0, Epsilon: 0.6625, Max Reward: 67.0]\n",
      "95 [Loss: 0.0331, Reward: 63.0, Epsilon: 0.6597, Max Reward: 67.0]\n",
      "96 [Loss: 0.0401, Reward: 22.0, Epsilon: 0.6569, Max Reward: 67.0]\n",
      "97 [Loss: 0.0327, Reward: 39.0, Epsilon: 0.6541, Max Reward: 67.0]\n",
      "98 [Loss: 0.0416, Reward: 23.0, Epsilon: 0.6514, Max Reward: 67.0]\n",
      "99 [Loss: 0.0343, Reward: 38.0, Epsilon: 0.6486, Max Reward: 67.0]\n",
      "100 [Loss: 0.0323, Reward: 70.0, Epsilon: 0.6459, Max Reward: 70.0]\n",
      "101 [Loss: 0.0271, Reward: 90.0, Epsilon: 0.6432, Max Reward: 90.0]\n",
      "102 [Loss: 0.0306, Reward: 43.0, Epsilon: 0.6404, Max Reward: 90.0]\n",
      "103 [Loss: 0.0276, Reward: 24.0, Epsilon: 0.6378, Max Reward: 90.0]\n",
      "104 [Loss: 0.0335, Reward: 31.0, Epsilon: 0.6351, Max Reward: 90.0]\n",
      "105 [Loss: 0.0292, Reward: 96.0, Epsilon: 0.6324, Max Reward: 96.0]\n",
      "106 [Loss: 0.0208, Reward: 21.0, Epsilon: 0.6297, Max Reward: 96.0]\n",
      "107 [Loss: 0.0325, Reward: 43.0, Epsilon: 0.6271, Max Reward: 96.0]\n",
      "108 [Loss: 0.0370, Reward: 78.0, Epsilon: 0.6245, Max Reward: 96.0]\n",
      "109 [Loss: 0.0287, Reward: 105.0, Epsilon: 0.6219, Max Reward: 105.0]\n",
      "110 [Loss: 0.0268, Reward: 103.0, Epsilon: 0.6193, Max Reward: 105.0]\n",
      "111 [Loss: 0.0180, Reward: 24.0, Epsilon: 0.6167, Max Reward: 105.0]\n",
      "112 [Loss: 0.0247, Reward: 40.0, Epsilon: 0.6141, Max Reward: 105.0]\n",
      "113 [Loss: 0.0495, Reward: 22.0, Epsilon: 0.6115, Max Reward: 105.0]\n",
      "114 [Loss: 0.0314, Reward: 80.0, Epsilon: 0.6090, Max Reward: 105.0]\n",
      "115 [Loss: 0.0271, Reward: 46.0, Epsilon: 0.6064, Max Reward: 105.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "116 [Loss: 0.0343, Reward: 68.0, Epsilon: 0.6039, Max Reward: 105.0]\n",
      "117 [Loss: 0.0491, Reward: 47.0, Epsilon: 0.6014, Max Reward: 105.0]\n",
      "118 [Loss: 0.0445, Reward: 37.0, Epsilon: 0.5989, Max Reward: 105.0]\n",
      "119 [Loss: 0.0530, Reward: 11.0, Epsilon: 0.5964, Max Reward: 105.0]\n",
      "120 [Loss: 0.0301, Reward: 26.0, Epsilon: 0.5939, Max Reward: 105.0]\n",
      "121 [Loss: 0.0423, Reward: 73.0, Epsilon: 0.5915, Max Reward: 105.0]\n",
      "122 [Loss: 0.0406, Reward: 37.0, Epsilon: 0.5890, Max Reward: 105.0]\n",
      "123 [Loss: 0.0341, Reward: 95.0, Epsilon: 0.5866, Max Reward: 105.0]\n",
      "124 [Loss: 0.0242, Reward: 45.0, Epsilon: 0.5841, Max Reward: 105.0]\n",
      "125 [Loss: 0.0282, Reward: 55.0, Epsilon: 0.5817, Max Reward: 105.0]\n",
      "126 [Loss: 0.0276, Reward: 46.0, Epsilon: 0.5793, Max Reward: 105.0]\n",
      "127 [Loss: 0.0284, Reward: 50.0, Epsilon: 0.5769, Max Reward: 105.0]\n",
      "128 [Loss: 0.0311, Reward: 28.0, Epsilon: 0.5746, Max Reward: 105.0]\n",
      "129 [Loss: 0.0406, Reward: 46.0, Epsilon: 0.5722, Max Reward: 105.0]\n",
      "130 [Loss: 0.0345, Reward: 63.0, Epsilon: 0.5698, Max Reward: 105.0]\n",
      "131 [Loss: 0.0312, Reward: 22.0, Epsilon: 0.5675, Max Reward: 105.0]\n",
      "132 [Loss: 0.0332, Reward: 41.0, Epsilon: 0.5652, Max Reward: 105.0]\n",
      "133 [Loss: 0.0238, Reward: 62.0, Epsilon: 0.5628, Max Reward: 105.0]\n",
      "134 [Loss: 0.0307, Reward: 22.0, Epsilon: 0.5605, Max Reward: 105.0]\n",
      "135 [Loss: 0.0368, Reward: 33.0, Epsilon: 0.5582, Max Reward: 105.0]\n",
      "136 [Loss: 0.0323, Reward: 12.0, Epsilon: 0.5560, Max Reward: 105.0]\n",
      "137 [Loss: 0.0345, Reward: 26.0, Epsilon: 0.5537, Max Reward: 105.0]\n",
      "138 [Loss: 0.0249, Reward: 19.0, Epsilon: 0.5514, Max Reward: 105.0]\n",
      "139 [Loss: 0.0330, Reward: 22.0, Epsilon: 0.5492, Max Reward: 105.0]\n",
      "140 [Loss: 0.0351, Reward: 89.0, Epsilon: 0.5469, Max Reward: 105.0]\n",
      "141 [Loss: 0.0435, Reward: 46.0, Epsilon: 0.5447, Max Reward: 105.0]\n",
      "142 [Loss: 0.0224, Reward: 15.0, Epsilon: 0.5425, Max Reward: 105.0]\n",
      "143 [Loss: 0.0393, Reward: 12.0, Epsilon: 0.5403, Max Reward: 105.0]\n",
      "144 [Loss: 0.0387, Reward: 45.0, Epsilon: 0.5381, Max Reward: 105.0]\n",
      "145 [Loss: 0.0367, Reward: 65.0, Epsilon: 0.5359, Max Reward: 105.0]\n",
      "146 [Loss: 0.0291, Reward: 24.0, Epsilon: 0.5337, Max Reward: 105.0]\n",
      "147 [Loss: 0.0285, Reward: 55.0, Epsilon: 0.5316, Max Reward: 105.0]\n",
      "148 [Loss: 0.0448, Reward: 34.0, Epsilon: 0.5294, Max Reward: 105.0]\n",
      "149 [Loss: 0.0356, Reward: 63.0, Epsilon: 0.5273, Max Reward: 105.0]\n",
      "150 [Loss: 0.0309, Reward: 33.0, Epsilon: 0.5251, Max Reward: 105.0]\n",
      "151 [Loss: 0.0370, Reward: 24.0, Epsilon: 0.5230, Max Reward: 105.0]\n",
      "152 [Loss: 0.0421, Reward: 50.0, Epsilon: 0.5209, Max Reward: 105.0]\n",
      "153 [Loss: 0.0419, Reward: 11.0, Epsilon: 0.5188, Max Reward: 105.0]\n",
      "154 [Loss: 0.0299, Reward: 40.0, Epsilon: 0.5167, Max Reward: 105.0]\n",
      "155 [Loss: 0.0381, Reward: 34.0, Epsilon: 0.5146, Max Reward: 105.0]\n",
      "156 [Loss: 0.0321, Reward: 25.0, Epsilon: 0.5126, Max Reward: 105.0]\n",
      "157 [Loss: 0.0316, Reward: 32.0, Epsilon: 0.5105, Max Reward: 105.0]\n",
      "158 [Loss: 0.0353, Reward: 26.0, Epsilon: 0.5085, Max Reward: 105.0]\n",
      "159 [Loss: 0.0401, Reward: 65.0, Epsilon: 0.5064, Max Reward: 105.0]\n",
      "160 [Loss: 0.0453, Reward: 30.0, Epsilon: 0.5044, Max Reward: 105.0]\n",
      "161 [Loss: 0.0377, Reward: 61.0, Epsilon: 0.5024, Max Reward: 105.0]\n",
      "162 [Loss: 0.0308, Reward: 79.0, Epsilon: 0.5004, Max Reward: 105.0]\n",
      "163 [Loss: 0.0256, Reward: 97.0, Epsilon: 0.4984, Max Reward: 105.0]\n",
      "164 [Loss: 0.0228, Reward: 88.0, Epsilon: 0.4964, Max Reward: 105.0]\n",
      "165 [Loss: 0.0356, Reward: 22.0, Epsilon: 0.4944, Max Reward: 105.0]\n",
      "166 [Loss: 0.0309, Reward: 38.0, Epsilon: 0.4924, Max Reward: 105.0]\n",
      "167 [Loss: 0.0331, Reward: 50.0, Epsilon: 0.4905, Max Reward: 105.0]\n",
      "168 [Loss: 0.0343, Reward: 61.0, Epsilon: 0.4885, Max Reward: 105.0]\n",
      "169 [Loss: 0.0361, Reward: 126.0, Epsilon: 0.4866, Max Reward: 126.0]\n",
      "170 [Loss: 0.0277, Reward: 36.0, Epsilon: 0.4847, Max Reward: 126.0]\n",
      "171 [Loss: 0.0199, Reward: 71.0, Epsilon: 0.4828, Max Reward: 126.0]\n",
      "172 [Loss: 0.0172, Reward: 93.0, Epsilon: 0.4808, Max Reward: 126.0]\n",
      "173 [Loss: 0.0212, Reward: 49.0, Epsilon: 0.4789, Max Reward: 126.0]\n",
      "174 [Loss: 0.0251, Reward: 89.0, Epsilon: 0.4771, Max Reward: 126.0]\n",
      "175 [Loss: 0.0160, Reward: 146.0, Epsilon: 0.4752, Max Reward: 146.0]\n",
      "176 [Loss: 0.0204, Reward: 87.0, Epsilon: 0.4733, Max Reward: 146.0]\n",
      "177 [Loss: 0.0132, Reward: 106.0, Epsilon: 0.4714, Max Reward: 146.0]\n",
      "178 [Loss: 0.0087, Reward: 37.0, Epsilon: 0.4696, Max Reward: 146.0]\n",
      "179 [Loss: 0.0113, Reward: 28.0, Epsilon: 0.4677, Max Reward: 146.0]\n",
      "180 [Loss: 0.0226, Reward: 108.0, Epsilon: 0.4659, Max Reward: 146.0]\n",
      "181 [Loss: 0.0226, Reward: 51.0, Epsilon: 0.4641, Max Reward: 146.0]\n",
      "182 [Loss: 0.0240, Reward: 77.0, Epsilon: 0.4623, Max Reward: 146.0]\n",
      "183 [Loss: 0.0154, Reward: 124.0, Epsilon: 0.4605, Max Reward: 146.0]\n",
      "184 [Loss: 0.0143, Reward: 72.0, Epsilon: 0.4587, Max Reward: 146.0]\n",
      "185 [Loss: 0.0130, Reward: 154.0, Epsilon: 0.4569, Max Reward: 154.0]\n",
      "186 [Loss: 0.0142, Reward: 89.0, Epsilon: 0.4551, Max Reward: 154.0]\n",
      "187 [Loss: 0.0107, Reward: 202.0, Epsilon: 0.4533, Max Reward: 202.0]\n",
      "188 [Loss: 0.0105, Reward: 188.0, Epsilon: 0.4516, Max Reward: 202.0]\n",
      "189 [Loss: 0.0125, Reward: 99.0, Epsilon: 0.4498, Max Reward: 202.0]\n",
      "190 [Loss: 0.0095, Reward: 180.0, Epsilon: 0.4481, Max Reward: 202.0]\n",
      "191 [Loss: 0.0076, Reward: 180.0, Epsilon: 0.4463, Max Reward: 202.0]\n",
      "192 [Loss: 0.0085, Reward: 65.0, Epsilon: 0.4446, Max Reward: 202.0]\n",
      "193 [Loss: 0.0112, Reward: 69.0, Epsilon: 0.4429, Max Reward: 202.0]\n",
      "194 [Loss: 0.0140, Reward: 157.0, Epsilon: 0.4412, Max Reward: 202.0]\n",
      "195 [Loss: 0.0127, Reward: 119.0, Epsilon: 0.4395, Max Reward: 202.0]\n",
      "196 [Loss: 0.0094, Reward: 69.0, Epsilon: 0.4378, Max Reward: 202.0]\n",
      "197 [Loss: 0.0184, Reward: 78.0, Epsilon: 0.4361, Max Reward: 202.0]\n",
      "198 [Loss: 0.0189, Reward: 61.0, Epsilon: 0.4344, Max Reward: 202.0]\n",
      "199 [Loss: 0.0142, Reward: 134.0, Epsilon: 0.4328, Max Reward: 202.0]\n",
      "200 [Loss: 0.0250, Reward: 17.0, Epsilon: 0.4311, Max Reward: 202.0]\n",
      "201 [Loss: 0.0221, Reward: 41.0, Epsilon: 0.4294, Max Reward: 202.0]\n",
      "202 [Loss: 0.0167, Reward: 110.0, Epsilon: 0.4278, Max Reward: 202.0]\n",
      "203 [Loss: 0.0142, Reward: 158.0, Epsilon: 0.4262, Max Reward: 202.0]\n",
      "204 [Loss: 0.0111, Reward: 16.0, Epsilon: 0.4245, Max Reward: 202.0]\n",
      "205 [Loss: 0.0122, Reward: 132.0, Epsilon: 0.4229, Max Reward: 202.0]\n",
      "206 [Loss: 0.0151, Reward: 140.0, Epsilon: 0.4213, Max Reward: 202.0]\n",
      "207 [Loss: 0.0124, Reward: 221.0, Epsilon: 0.4197, Max Reward: 221.0]\n",
      "208 [Loss: 0.0179, Reward: 38.0, Epsilon: 0.4181, Max Reward: 221.0]\n",
      "209 [Loss: 0.0116, Reward: 113.0, Epsilon: 0.4165, Max Reward: 221.0]\n",
      "210 [Loss: 0.0155, Reward: 140.0, Epsilon: 0.4149, Max Reward: 221.0]\n",
      "211 [Loss: 0.0160, Reward: 147.0, Epsilon: 0.4134, Max Reward: 221.0]\n",
      "212 [Loss: 0.0107, Reward: 68.0, Epsilon: 0.4118, Max Reward: 221.0]\n",
      "213 [Loss: 0.0109, Reward: 116.0, Epsilon: 0.4103, Max Reward: 221.0]\n",
      "214 [Loss: 0.0110, Reward: 134.0, Epsilon: 0.4087, Max Reward: 221.0]\n",
      "215 [Loss: 0.0108, Reward: 231.0, Epsilon: 0.4072, Max Reward: 231.0]\n",
      "216 [Loss: 0.0081, Reward: 207.0, Epsilon: 0.4056, Max Reward: 231.0]\n",
      "217 [Loss: 0.0045, Reward: 353.0, Epsilon: 0.4041, Max Reward: 353.0]\n",
      "218 [Loss: 0.0059, Reward: 215.0, Epsilon: 0.4026, Max Reward: 353.0]\n",
      "219 [Loss: 0.0041, Reward: 262.0, Epsilon: 0.4011, Max Reward: 353.0]\n",
      "220 [Loss: 0.0065, Reward: 319.0, Epsilon: 0.3996, Max Reward: 353.0]\n",
      "221 [Loss: 0.0061, Reward: 57.0, Epsilon: 0.3981, Max Reward: 353.0]\n",
      "222 [Loss: 0.0151, Reward: 51.0, Epsilon: 0.3966, Max Reward: 353.0]\n",
      "223 [Loss: 0.0141, Reward: 180.0, Epsilon: 0.3951, Max Reward: 353.0]\n",
      "224 [Loss: 0.0153, Reward: 106.0, Epsilon: 0.3937, Max Reward: 353.0]\n",
      "225 [Loss: 0.0116, Reward: 83.0, Epsilon: 0.3922, Max Reward: 353.0]\n",
      "226 [Loss: 0.0144, Reward: 112.0, Epsilon: 0.3907, Max Reward: 353.0]\n",
      "227 [Loss: 0.0103, Reward: 232.0, Epsilon: 0.3893, Max Reward: 353.0]\n",
      "228 [Loss: 0.0039, Reward: 118.0, Epsilon: 0.3878, Max Reward: 353.0]\n",
      "229 [Loss: 0.0075, Reward: 127.0, Epsilon: 0.3864, Max Reward: 353.0]\n",
      "230 [Loss: 0.0075, Reward: 138.0, Epsilon: 0.3850, Max Reward: 353.0]\n",
      "231 [Loss: 0.0086, Reward: 57.0, Epsilon: 0.3836, Max Reward: 353.0]\n",
      "232 [Loss: 0.0057, Reward: 10.0, Epsilon: 0.3821, Max Reward: 353.0]\n",
      "233 [Loss: 0.0102, Reward: 67.0, Epsilon: 0.3807, Max Reward: 353.0]\n",
      "234 [Loss: 0.0062, Reward: 434.0, Epsilon: 0.3793, Max Reward: 434.0]\n",
      "235 [Loss: 0.0045, Reward: 250.0, Epsilon: 0.3779, Max Reward: 434.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236 [Loss: 0.0045, Reward: 319.0, Epsilon: 0.3766, Max Reward: 434.0]\n",
      "237 [Loss: 0.0066, Reward: 174.0, Epsilon: 0.3752, Max Reward: 434.0]\n",
      "238 [Loss: 0.0087, Reward: 116.0, Epsilon: 0.3738, Max Reward: 434.0]\n",
      "239 [Loss: 0.0061, Reward: 292.0, Epsilon: 0.3724, Max Reward: 434.0]\n",
      "240 [Loss: 0.0067, Reward: 227.0, Epsilon: 0.3711, Max Reward: 434.0]\n",
      "241 [Loss: 0.0076, Reward: 139.0, Epsilon: 0.3697, Max Reward: 434.0]\n",
      "242 [Loss: 0.0088, Reward: 213.0, Epsilon: 0.3684, Max Reward: 434.0]\n",
      "243 [Loss: 0.0082, Reward: 120.0, Epsilon: 0.3670, Max Reward: 434.0]\n",
      "244 [Loss: 0.0064, Reward: 18.0, Epsilon: 0.3657, Max Reward: 434.0]\n",
      "245 [Loss: 0.0096, Reward: 373.0, Epsilon: 0.3644, Max Reward: 434.0]\n",
      "246 [Loss: 0.0047, Reward: 314.0, Epsilon: 0.3631, Max Reward: 434.0]\n",
      "247 [Loss: 0.0110, Reward: 51.0, Epsilon: 0.3618, Max Reward: 434.0]\n",
      "248 [Loss: 0.0133, Reward: 248.0, Epsilon: 0.3604, Max Reward: 434.0]\n",
      "249 [Loss: 0.0032, Reward: 192.0, Epsilon: 0.3591, Max Reward: 434.0]\n",
      "250 [Loss: 0.0076, Reward: 172.0, Epsilon: 0.3579, Max Reward: 434.0]\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    dqn = DeepQNetwork(env_name='CartPole-v1',\n",
    "                        epsilon=0.9, \n",
    "                        gamma=0.8, \n",
    "                        decay_rate=0.005, \n",
    "                        min_epsilon=0.1)\n",
    "\n",
    "    # Model builder\n",
    "    def model(n_inputs, n_outputs):    \n",
    "        clf = NeuralNetwork(optimizer=Adam(), loss=SquareLoss)\n",
    "        clf.add(Dense(64, input_shape=(n_inputs,)))\n",
    "        clf.add(Activation('relu'))\n",
    "        clf.add(Dense(n_outputs))\n",
    "        return clf\n",
    "\n",
    "    dqn.set_model(model)\n",
    "\n",
    "    print ()\n",
    "    dqn.model.summary(name=\"Deep Q-Network\")\n",
    "\n",
    "    dqn.train(n_epochs=500)\n",
    "    dqn.play(n_epochs=100)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
